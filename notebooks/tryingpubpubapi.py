# -*- coding: utf-8 -*-
"""tryingpubpubapi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15gSHFLqHubhtCJH-uW5z5R9g6I-57-th
"""

#pip install git+https://github.com/unjournal/pypubpub@main

# This was to bring in the (below?) code from Github

!pip install pycryptodome
import requests
import json
from Crypto.Hash import keccak #need hashed passwords for API

"""## 'Classes' (below)

Nesim built this code for The Unjournal.
To interact with PubPub V6 (and possibly V7)  through API.

\

Functionality:

I. Log in and get an API token (lasts ~2 hours)

II. Make API requests...
  1. List collections
  2. List pubs
  3. Search for pubs/authors with search criteria
  4. Make new pubs, delete pubs
  5. Update some metadata
  6. Some text inserts and manipulation of pub content

See the pubpub API docs: https://www.pubpub.org/apiDocs#/
"""

from sys import ps1
class Pubshelper:
    def __init__(self, community_url="https://unjournal.pubpub.org", community_id="d28e8e57-7f59-486b-9395-b548158a27d6", email='revwr@gmail.com',password = '8paswortt' ):
        self.community_url = community_url
        self.community_id = community_id
        self.cookieJar = None
        self.logged_in = False
        self.email = email
        self.password = password


    def login(self):
        k = keccak.new(digest_bits=512)
        k.update(self.password.encode())
        response = requests.post(
            url=f'{self.community_url}/api/login',
            headers={ "accept": "application/json",
                      "cache-control": "no-cache",
                      "content-type": "application/json",
                      "pragma": "no-cache"
                     },
            data=json.dumps({
                'email': self.email,
                'password': k.hexdigest(),

            }),
        )

        self.cookieJar=response.cookies


        if not self.cookieJar:
            raise Exception(f'Login failed, with status {response.status_code}: {response.text}')

        self.logged_in = True

    def authed_request(self, path, method='GET', body=None, options=None, additionalHeaders=None):

            response = requests.request(
                method,
                f'{self.community_url}/api/{path}',
                data=json.dumps(body) if body else None,
                cookies=self.cookieJar,
                headers={ "accept": "application/json",
                      "cache-control": "no-cache",
                      "content-type": "application/json",
                      "pragma": "no-cache",
                      "origin": self.community_url,
                      **(additionalHeaders if additionalHeaders else {})
                     },
                **options if options else {},
            )

            if response.status_code < 200 or response.status_code >= 300:
                raise Exception(
                    f'Request failed with status {response.status_code}: {response.text}'
                )

            return response.json()

    def logout(self):
        response =  self.authed_request('logout', 'GET')
        self.cookieJar = None
        self.logged_in = False
        print('Succesfully logged out')

    def getPubByIdorSlug(self, slugOrId, attributes=None, include=None ):

        response = self.authed_request(
            f'pubs/{slugOrId}',
            # todo add url query params for `attribute` and `include` - see https://www.pubpub.org/apiDocs#/paths/api-pubs-slugOrId/get
            'GET'
            )

        return response


    def connect_pub(self, srcPubId, targetPubId,
                    relationType="supplement",
                     pubIsParent=False,
                     approvedByTarget=True,
                     rank=None
                     ):

        response = self.authed_request(
            'pubEdges',
            'POST',
            {
                "pubId": srcPubId,
                "relationType": relationType,
                "pubIsParent": pubIsParent,
                "approvedByTarget": approvedByTarget,
                "targetPubId": targetPubId,
                **( {'rank': rank} if rank else {}),
                #"externalPublication": null
            },
        )
        return response

    ###
    # Note: To connect to an external article, you can first try the connect_pub method. Many external resources have a
    # Pubpub Id. So you can use the connect_pub method after looking up its Pubpub Id.
    #
    #  This method can be used when an external resource does not yet have a Pubpub Id. After using this method to connect
    # to an external resource, the external resource seems to get a Pubpub Id assigned to it, which could be used in the future.
    ###
    def connect_pub_to_external(self, srcPubId,
                                title,
                                url,
                                publicationDate,
                                description,
                                doi=None,
                                avatar=None,
                                contributors=[],
                                relationType="review",
                     rank=None,
                     pubIsParent=False,
                     approvedByTarget=True
                     ):

        response = self.authed_request(
            'pubEdges',
            'POST',
            {
                "pubId": srcPubId,
                "relationType": relationType,
                "pubIsParent": pubIsParent,
                "approvedByTarget": approvedByTarget,
                # "targetPubId": targetPubId,
                **( {'rank': rank} if rank else {}),
                "externalPublication": {
                    "avatar": avatar,
                    "contributors": contributors,
                    "doi": doi,
                    "url": url,
                    "publicationDate": publicationDate, #"2024-04-01T00:00:00.000Z",
                    "title": title,
                    "description": description
                    },
            }
        )
        return response

    ###
    # To disconnnect 2 pubs, this might be done before deleting them as part of cleanup
    #
    # question: do we need other helper methods to remove connections? such as deleteAllEdgesforPubId?
    ###
    def disconnect_pub(self, edgeId=None, srcPubId=None, targetPubId=None ):
        if edgeId:
          pass
        elif srcPubId and targetPubId:
          # edgeId = self.getTheEdgeIdbyPubIds(srcPubId, targetPubId)
          pass
        else:
          raise Exception('Bad arguments. Either pass in the edgeId to delete, or pass in src and target Pub Ids')

        response = self.authed_request(
            'pubEdges',
            'DELETE',
             {
                 "pubEdgeId": edgeId
              },
        )
        return response


    def get_many_pubs(self, limit = 50, offset = 0, ordering= {'field': 'updatedDate', 'direction': 'DESC'}, collection_ids=None, pub_ids=None):

        response = self.authed_request(
            'pubs/many',
            'POST',
            {
                'alreadyFetchedPubIds': [],
                'pubOptions': {
                    'getCollections': True,
                },
                'query': {
                    'communityId': self.community_id,
                    **( {'collectionIds': collection_ids} if collection_ids else {}),
                    **(   {'withinPubIds': pub_ids} if pub_ids else {}),
                    'limit': limit,
                    'offset': offset,
                    'ordering': ordering,
                },
            },
        )

        return response

    def get_byreleased_pubs(self,
                            limit = 50,
                            offset = 0,
                            ordering= {'field': 'updatedDate', 'direction': 'DESC'} ,
                            isReleased=False,
                            collection_ids=None ,
                            pub_ids=None,
                            alreadyFetchedPubIds=[],
                            relatedUserIds =[]
                            ):

        response = self.authed_request(
            'pubs/many',
            'POST',
            {
                'alreadyFetchedPubIds': alreadyFetchedPubIds,
                'pubOptions': {
                    'getCollections': True,
                },
                'query': {
                    'communityId': self.community_id,
                    isReleased: isReleased,
                    **( {'collectionIds': collection_ids} if collection_ids else {}),
                    **(   {'withinPubIds': pub_ids} if pub_ids else {}),
                    'limit': limit,
                    'offset': offset,
                    'ordering': ordering,
                    **(   {'relatedUserIds': relatedUserIds} if relatedUserIds else {}),
                },
            },
        )

        return response

    def create_pub(self, slug,title , description):
        response =  self.authed_request(
            path='pubs',
            method='POST',
            body={
                # 'pubId': pub_id,
                "slug":slug,
                "title": title,
                "description" : description,
                'communityId': self.community_id,
            },
        )
        return response


    def create_pub_02(self, slug,title , description):
        response =  self.authed_request(
            path='pubs',
            method='POST',
            body={
                # 'pubId': pub_id,
                # "slug":slug,
                # "title": title,
                # "description" : description,
                'communityId': self.community_id,
            },
        )
        return response

    def delete_pub(self, pub_id):
        response =  self.authed_request(
            path='pubs',
            method='DELETE',
            body={
                'pubId': pub_id,
                'communityId': self.community_id,
            },
        )
        return response

    def getPub(self, pub_id):
        return self.get_many_pubs(pub_ids=[pub_id])

"""Use the Google Colab Notebook "Secrets" menu option to store and use the user password. Look for the icon of a key at left.

Other options for storing and entering passwords are mentioned on SO:
- [here](https://stackoverflow.com/a/54577734)
- and [here](https://stackoverflow.com/questions/54571009/how-to-hide-secret-keys-in-google-colaboratory-from-users-having-the-sharing-lin)

"""

!pip install google-auth
import google.auth
creds, _ = google.auth.default()

from google.colab import userdata
username="contact@unjournal.org"
#contact_uj_password = userdata.get('contact_uj_password')
uj_helper = Pubshelper(community_url="https://unjournal.pubpub.org", community_id="d28e8e57-7f59-486b-9395-b548158a27d6",
                       email=username, password = contact_uj_password)

# temporary DHJ: you can delete
pw = ""
k = keccak.new(digest_bits=512)
k.update(pw.encode())
print(k.hexdigest())

"""*You need to run the block below to make any calls*"""

# login and get a cookie to make authorized calls
uj_helper.login()

# troubleshooting purposes only: view the settings for the user, and look at the cookie for fun (which should not be necessary so maybe)
uj_helper.__dict__

help(uj_helper)

"""# Extract and show some pub content, settings, and metadata"""

uj_pub_list = uj_helper.get_many_pubs(limit = 100) #this stores it locally

print(uj_pub_list.keys()) # view the available datafields in the response

print(uj_pub_list["pubIds"]) # view the pubIds in the response

print(uj_pub_list["pubsById"]) # view the pubIds in the response

"""Above, `pubIds` are the unique identifier for the pubs

`pubsByID` seems to have all the metadata

Here are the fields available for a `pubsByID` entry:

```
'id', 'slug', 'title', 'htmlTitle', 'description', 'htmlDescription', 'avatar', 'customPublishedAt', 'doi', 'labels', 'downloads', 'metadata', 'viewHash', 'editHash', 'reviewHash', 'commentHash', 'draftId', 'communityId', 'crossrefDepositRecordId', 'scopeSummaryId', 'createdAt', 'updatedAt', 'members', 'draft', 'crossrefDepositRecord', 'scopeSummary', 'reviews', 'releases', 'attributions', 'collectionPubs', 'outboundEdges', 'inboundEdges', 'isRelease', 'releaseNumber'
```
"""

#convert the dict to a list of tuple pairs, and then access the first pair to view the contents
list(uj_pub_list['pubsById'].items())[20]

# you can also access the entries in the dict by using a pubId as a key, and then specific fields as a key, since the datastructure is a dict pointing to sub dict s
print(uj_pub_list['pubsById']['b9775e89-5a43-4a6e-a69c-36b23067426b']['title'])

uj_pub_list['pubsById']['b9775e89-5a43-4a6e-a69c-36b23067426b']['attributions'][1]

uj_pub_list['pubsById']['b9775e89-5a43-4a6e-a69c-36b23067426b'].keys()

# find 'connections' and other things

# assuming the particular pubId was returned in the response object and was not cut off by the limit
pubId = "f5b7e124-ba77-4aa5-8955-32fe77c51350"
ai_eval_pub = uj_pub_list['pubsById'][pubId]

inbound_relations = ai_eval_pub["inboundEdges"]
outbound_relations = ai_eval_pub["outboundEdges"]

# use json lib to pretty print the results
import json
inb = json.dumps(inbound_relations, indent=4)
outb = json.dumps(outbound_relations, indent=4)

print('inbound:', inb)
print("outbound", outb)

#print titles of related pubs, using results of previous cell
# related_pubs = [outbound_relations["targetPub"][k] for k,v in ]

# [dict[key] for key in (tuple_of_searched_keys)]
# new_list = [(k, v) for k, v in d.items()]
print(" === Related Outbound Pub Titles ===", len(outbound_relations))
for edge in outbound_relations:
  print("edge::",edge)
  if 'targetPub' in edge and edge['targetPub']:
    title = edge['targetPub'][ "title"]
  elif 'externalPublication' in edge:
    title = 'externalPublication: ' + edge['externalPublication']['title']
  print(title)

"""# Search for Pub"""

# awaiting for information on how to search by title and/or content from pubpub or github discussions
# would use an algolia key and the algolia API url: https://v4f3dk64un-dsn.algolia.net/1/indexes/pubs/query?x-algolia-agent=Algolia%20for%20JavaScript%20(4.20.0)%3B%20Browser
#
# will keep you posted!

# This is a 'fuzzy search' algorithm

#
# another possible url to search for a pub (and additionaly external papers)
# https://unjournal.pubpub.org/api/pubEdgeProposal?object=https%3A%2F%2Fwww.nber.org%2Fpapers%2Fw25589
#
# the above url has parameter "object" which is set to either a url or doi. although i did nt have success with the doi search yet

### get one pub by id
#. 3 helper methods that send a request to the Pubpub backend:
#. - getPub(self, pub_id)
#. - get_many_pubs(pub_ids) - takes a list array of pubIds, so could be one or more at a time
#  - getPubByIdorSlug(self, slugOrId )
###
import json # import again, just in case we skipped a step, to help pretty print the responses

pubId = "f5b7e124-ba77-4aa5-8955-32fe77c51350"

pub_getPub = uj_helper.getPub(pubId)
pub_many_pubs = uj_helper.get_many_pubs(pub_ids=[pubId])
pub_getby_pubId = uj_helper.getPubByIdorSlug(pubId)  # by PubId
pub_getby_slug = uj_helper.getPubByIdorSlug('alatasevalsum')  # by SlugId

print(json.dumps(pub_getby_slug, indent=4))

"""# Add and delete pub(s)

"""

# Note slug has to be unique, so if you execute many times it will error if the slug is not updated

slug = 'test-slug-445'
title = 'test title 00'
description = 'test description 00'
test_pub_01 = uj_helper.create_pub(slug=slug, title= title, description= description)
print(test_pub_01)

### Demonstrate the pub was created in the previous step and then delete it #
# 1. get the pub by slug or Id

test_pub_01_02 = uj_helper.getPubByIdorSlug(test_pub_01['id'])
print(f'we found the new pub by pubId::  id  and slug: ', test_pub_01_02['id'], test_pub_01_02['slug'])
print(' === now we delete it')
uj_helper.delete_pub(test_pub_01_02['id'])

try:
  uj_helper.getPubByIdorSlug(test_pub_01['id'])
except BaseException as exception:
  print(exception)

# create 2 pubs and connect them to each other
# see - https://www.pubpub.org/apiDocs#/paths/api-pubEdges/post
pub_01 = uj_helper.create_pub(slug='test-g-colab-01', title= "test title 01", description= "test description 01")
pub_02 = uj_helper.create_pub(slug='test-g-colab-02', title= "comment on test title 02", description= "test description 02")

srcPubId = pub_01['id']
targetPubId = pub_02['id']

edge = uj_helper.connect_pub(srcPubId, targetPubId,
                    relationType="supplement",
                     pubIsParent=False,
                     approvedByTarget=True
                     )

import json
print('edge: ', json.dumps(edge))

# cleanup: disconnect pubs
uj_helper.disconnect_pub(edgeId=edge['id'])

# clean up
uj_helper.delete_pub(pub_01['id'])
uj_helper.delete_pub(pub_02['id'])

# import uuid

# u0=  uuid.uuid4()
# u1=  str(u0)
tester.create_pub(slug="u1-apr3", title='test', description='test description 00')

"""# Adjust settings/metadata for pubs




"""



"""# Export/import content from Coda etc.




"""



"""# Code steps for creating a new package of connected pubs

I'll flesh this out in [this Gdoc](https://docs.google.com/document/d/1F9w46tN3u8eeE8f5iTi543eDxAf9BTJ1OXJgxDHtPsw/edit)


0. Capture/find content from V7

To see this,
- See content under [stages](https://v7.pubpub.org/c/unjournal/stages)

- For relevant paper, click 'Contents' and then 'Evaluation and summary' for each evaluation

- This displays the structured data (the metrics), and comments.

- The evaluation discussion is found under 'Upload your evaluation' as a file (or given in a link in 'please provide a concise summary')


1. Create pub in V6


2. Add metadata (author etc)
3. Add content
4. Format content
5. Create connections to other pubs and to original data
6. Request DOI
7. Set to publish
8. Extract content (DOI, etc) for input into REPEC

# Useful checkin and cleanup operations

1. Show me a list of all pubs by 'title'
2. Delete all pubs with a certain string in the title (e.g., 'untitled')
3. Create a clean table of all pubs by title and a few important metadata elements
4. Change all metadata elements 'with a conditional' ... e.g., "if the author has the name 'Anonymous' change it to say 'Anonymous Author'  
5. Bonus: Export a clean selection of metadata (or push it somewhere)
"""

# prompt: do an API call to get all pubs. Next show me a list of the title and id of all pubs from the API response,  sort and display it as a very well formatted  table. I only want to see the title and id. Format it as a table with equal space to line everything up. Put in a comma to separate fields

import pandas as pd
import json

# Get all pubs
pubs = uj_helper.get_many_pubs(limit=100)

# Extract the title and id of all pubs
pub_data = [(pub["title"], pub["id"]) for pub in pubs["pubsById"].values()]

# Create a pandas DataFrame
df = pd.DataFrame(pub_data, columns=["Title", "ID"])

# Sort the DataFrame by title
df = df.sort_values("Title")

# Display the DataFrame as a table
print(df.to_string(index=False))

# prompt: show me a list of the title and id of all pubs in uj_pub_list,
# sort and display it as a nice table. I only want to see the title and id

import pandas as pd

# Create a dataframe from the pubsById dictionary
df = pd.DataFrame(uj_pub_list['pubsById']).T

# Select only the title and id columns
df = df[['title', 'id']]

# Sort the dataframe by title
df = df.sort_values(by=['title'])

# Display the dataframe
print(df.to_string())

def delete_pubs_with_title(title_substring):
  """
  Deletes all pubs with a title that contains the given substring.

  Args:
    title_substring: The substring to search for in pub titles.
  """

  # Get a list of all pub IDs.
  pubs = uj_helper.get_many_pubs(limit=500)
  pub_ids = pubs['pubIds']

  # Iterate through the pub IDs and delete any pubs with the given substring in the title.
  for pub_id in pub_ids:
    pub = pubs['pubsById'][pub_id]
    if 'title' in pub and title_substring in pub['title']:
      print(pub['title'])  # Print the title of the pub
      uj_helper.delete_pub(pub_id)
      print(f'deleting pub id {pub_id} with title', pub['title'])

delete_pubs_with_title('Untitled')

slug = 'guest'
title = "Evaluation Summary and Metrics: 'The Benefits and Costs of Guest Worker Programs: Experimental Evidence from the India-UAE Migration Corridor'"
description = "Evaluation Summary and Metrics: 'The Benefits and Costs of Guest Worker Programs: Experimental Evidence from the India-UAE Migration Corridor' for The Unjournal. Evaluators: Anonymous for sharing with authors"
evalguest = uj_helper.create_pub(slug=slug, title= title, description= description)
print(evalguest)

title_g = "The Benefits and Costs of Guest Worker Programs: Experimental Evidence from the India-UAE Migration Corridor"

slug = 'gueste2'
title = "Evaluation 1 of " + title_g

description = "Evaluation of {title_g} for The Unjournal. Evaluator: Anonymous for now"
evalguest1 = uj_helper.create_pub(slug=slug, title= title, description= description)

title_g = "The Benefits and Costs of Guest Worker Programs: Experimental Evidence from the India-UAE Migration Corridor"

slug = 'gueste02'
title = "Evaluation 2 of " + title_g

description = "Evaluation 2 of {title_g} for The Unjournal. Evaluator: Anonymous for now"
evalguest1 = uj_helper.create_pub(slug=slug, title= title, description= description)

"""# Concerns
(To ask PubPUb etc)

Worried about unrecoverable changes ... Restore points
- Changing metadata everywhere (e.g., 'author')
- Changing content in pubs (text, widgets etc.)
- Deleting pubs

"""