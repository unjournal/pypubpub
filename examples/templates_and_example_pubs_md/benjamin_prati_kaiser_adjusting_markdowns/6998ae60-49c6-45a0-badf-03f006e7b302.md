---
affiliation:
- id: 0
  organization: Warwick Business School
article:
  elocation-id: e1heterogenity
author:
- Caspar Kaiser
bibliography: /tmp/tmp-60DSDgS8nrZ5LY.json
copyright:
  link: "https://creativecommons.org/licenses/by/4.0/"
  text: Creative Commons Attribution 4.0 International License
  type: CC-BY
csl: /app/dist/server/server/utils/citations/citeStyles/apa-6th-edition.csl
journal:
  publisher-name: The Unjournal
  title: The Unjournal
link-citations: true
title: Evaluation 1 of "Adjusting for Scale-Use Heterogeneity in
  Self-Reported Well-Being"
uri: "https://unjournal.pubpub.org/pub/e1heterogenity"
---

# Abstract 

This is a major methodological innovation in how we can adjust for
differences in scale-use. The empirical component would especially
benefit from more diverse and reliable samples.

# Summary Measures

We asked evaluators to give some overall assessments, in addition to
ratings across a range of criteria. *See the *[*evaluation summary
"metrics"*](https://unjournal.pubpub.org/pub/evalsumheterogenity#metrics "null")*
for a more detailed breakdown of this. See these ratings in the context
of all Unjournal ratings, with some analysis, in our *[*data
presentation
here.*](https://unjournal.github.io/unjournaldata/chapters/evaluation_data_analysis.html#basic-presentation "null")[^1]*
*

+-------------------+-------------------+---+
|                   | **Rating**        | * |
|                   |                   | * |
|                   |                   | 9 |
|                   |                   | 0 |
|                   |                   | % |
|                   |                   | C |
|                   |                   | r |
|                   |                   | e |
|                   |                   | d |
|                   |                   | i |
|                   |                   | b |
|                   |                   | l |
|                   |                   | e |
|                   |                   | I |
|                   |                   | n |
|                   |                   | t |
|                   |                   | e |
|                   |                   | r |
|                   |                   | v |
|                   |                   | a |
|                   |                   | l |
|                   |                   | * |
|                   |                   | * |
+===================+===================+===+
| **Overall         | 95/100            | 8 |
| assessment **     |                   | 0 |
|                   |                   |   |
|                   |                   | - |
|                   |                   | 1 |
|                   |                   | 0 |
|                   |                   | 0 |
+-------------------+-------------------+---+
| **Journal rank    | 4.7/5             | 4 |
| tier, normative   |                   | . |
| rating**          |                   | 1 |
|                   |                   |   |
|                   |                   | - |
|                   |                   | 5 |
+-------------------+-------------------+---+

**Overall assessment **(See footnote[^2])

**Journal rank tier, normative rating (0-5): ** On a 'scale of
journals', what 'quality of journal' should this be published in?[^3]
*Note: 0= lowest/none, 5= highest/best. *

# Claim identification and assessment [^4]

## I. Identify the most important and impactful factual claim this research makes[^5] {#i-identify-the-most-important-and-impactful-factual-claim-this-research-makes}

There is a new method to adjust for scale-use differences which can be
implemented both with existing data containing vignettes, and with new
data that require only a few additional questions.

## II. To what extent do you \*believe\* the claim you stated above?[^6] {#ii-to-what-extent-do-you-believe-the-claim-you-stated-above}

There certainly is a new method. I would be interested in seeing
comparisons with existing methods.

# Written report

## Overview

An important question in subjective wellbeing research is whether
responses are interpersonally (and intra-personally) comparable. This is
especially critical for regression analyses of wellbeing data. If
differences in scale-use are correlated with covariates of interest,
then failing to correct for such differences may bias coefficient
estimates.

This paper [@n1zozkwjgu8] addresses this and related concerns in two
main ways. First, it proposes a new framework for thinking about
scale-use heterogeneity and develops econometric approaches to adjust
for such differences. Second it provides empirical estimates of
scale-use differences and their consequences for empirical work.

I will first comment on the methodological contribution and then (more
briefly) on the empirical part.

## Methodological Contribution

As the authors acknowledge, the fundamental idea of the paper has
antecedents in the use of vignettes (King et al., 2004 [@nllztkgj2ee])
and the literature on response styles. However, the treatment of the
issue is more rigorous and (arguably) relies on weaker assumptions than
previous approaches.

The core idea is to ask respondents a series of '*calibration
questions*' (which may be wellbeing-specific or more general) about
which we assume that perceptions are shared across respondents. By
observing differences in responses---which must be due to either
scale-use differences or measurement error---we can then infer and
correct for differences in scale use.

### Comments on Assumptions

Throughout, the paper relies on four key assumptions, stated roughly as
follows:

1.  **Affine transformation:** The scale-use of person A is a positive
    affine transformation of the scale-use of person B.

2.  **Common perception:** Each calibration question is perceived in the
    same way across respondents.

3.  **Response consistency:** The functions that translate scale-use
    between respondents are the same across questions.

4.  **Independent errors:** Errors in both reporting and the perception
    of calibration questions are independent.

There are additional assumptions, as noted in Table 5, but I will not
focus on these here.

*Should we believe these assumptions?* On the basis of Online Appendix
B, where the authors extend the translation function with a quadratic
term and provide graphical evidence, I am convinced that **Assumption
1** approximately holds. In principle, Assumption 1 could be replaced
with weaker assumptions (such as merely assuming monotonicity of the
function relating scale-use between respondents). This would only come
at the cost of more complicated econometrics and the need for additional
calibration questions, but there do not seem \[to be\] deep conceptual
hurdles.

I have no strong intuitions about **Assumption 4** and the additional
econometric assumptions, though they all seem broadly plausible, and the
authors give some evidence to support them (e.g. on zero co-skewness).

Concerning **Assumptions 2** and **3**, the authors hint at an important
trade-off: More 'objective' calibration questions (like the darkness of
a circle) are more likely to satisfy Assumption 2, but are less likely
to satisfy Assumption 3. Conversely, calibration questions that are most
directly concerned with the quantity of interest (here, subjective
wellbeing)---such as traditional vignettes---are more likely to satisfy
Assumption 3 but less likely to satisfy Assumption 2.

I am uncertain whether questions can be designed that jointly satisfy
both assumptions. However, it may be the case that estimates of the bias
from scale-use differences are bounded between the two extremes. That
is, the true translation function may lie somewhere between the function
estimated from objective (say, visual) calibration questions and the
function estimated from a set of traditional vignettes. I am not certain
about this, but I encourage the authors to investigate.

As a more simple exercise, it would be valuable to demonstrate the
sensitivity of estimates such as those shown in Table 6 (which, for me,
presents the most important substantive empirical result) to varying the
included set of calibration questions. If we see few differences in
results, this should suggest that Assumptions 2 and 3 are approximately
met. Indeed similar such tests have been proposed in the vignette
literature. See e.g.: (d'Uva et al., 2011 [@nt21871zmkp]).

### Relationship to HOPIT Model and Broader Considerations

The hierarchical ordered probit ('HOPIT') model (and variants thereof)
is currently the standard method with which researchers seek to adjust
for differences in scale-use. Principally, that model relies on assuming
*vignette equivalence* (which roughly maps onto this paper's Assumption
2), *response consistency* (roughly corresponding to this paper's
Assumption 3), and conditional normality of the underlying latent
construct of interest.

Although I appreciate that in this paper the strong assumption of
conditional normality is dropped, the assumptions of the HOPIT model are
not, strictly speaking, weaker or stronger than the set of assumptions
required in this paper. Moreover, the HOPIT approach is designed for
ordinal variables with few categories, whereas this paper's approach is
designed for quasi-continuous dependent variables.

Given the prominence of the HOPIT approach in the literature, it would
therefore be useful to have: (1) a theoretical clarification of the
similarities and differences between the two approaches (perhaps they
can in some way be nested or combined?); and (2) an investigation into
how large the differences in empirical results are between the types of
approaches, for example in terms of ratios of estimated coefficients in
regression models.

I do not know enough about the literature on response styles to make
concrete suggestions, but I imagine similar points could be made about
clarifying the connections between this paper's approach and standard
methods used in that field.

### Miscellaneous

**Objective validation:** The fact that correcting for scale-use
differences increases the extent to which objectively observable
quantities (like height and weight, but also pollution levels) are
predicted by the subjective questions is really quite convincing. One
question that may be useful to answer: Suppose we simply choose for each
respondent that linear transformation of the raw subjective data which
maximizes how well the objective data are predicted. How close would
these functions be to the estimated translation functions?

**Accessibility:** The authors are extremely detailed and thorough in
their reasoning, which is appreciated. However, this does come at some
cost to accessibility. In my view, the most important application lies
in correcting estimates of regression coefficients for scale-use
differences. I would structure the paper around this goal, focusing
first on the method-of-moments estimator, with the more sophisticated
estimators relegated to an appendix. It would also be helpful if R
and/or Stata packages accompanied the paper.

**Persistence and Panel Data** The authors briefly discuss persistence
of scale-use differences. I think this is crucial. Analyses of wellbeing
data increasingly rely on panels with individual fixed effects. If the
shifter is time-invariant, we need not worry about it when using
individual fixed effects. However, as the authors briefly note, if the
shifter varies over time and with covariates of interest, estimates from
fixed effects models will still be biased. And, of course, fixed effects
do nothing to address the stretcher. I would therefore encourage the
authors to work out how to extend the proposed method to the case of an
individual fixed effects model.

## Empirical Contribution

Two substantive empirical results in the paper are that (1) after
correcting, mean wellbeing is higher, and that (2) the variance in
wellbeing is smaller when correcting for scale-use differences compared
to naïve analyses. Given that there is no *absolute* scale of wellbeing,
I am unsure whether we can substantively conclude from this that
wellbeing is generally higher and less unequally distributed than naïve
analyses would suggest. (The authors do not make this claim explicitly,
but it is a potentially noteworthy implication.)

However, in my view, the most important empirical result of the paper is
what is shown in Table 6. From it, I take the conclusion that the extent
to which scale-use differences affect estimates of the determinants of
life satisfaction is rather small (and similar, perhaps slightly more
pronounced, for anxiety). The authors do not emphasize this result
greatly, but if true, this would be really good news for the wider
field!

*Can we trust these results?* This depends partly on the validity of the
assumptions discussed above. But it also depends on the sample. The
empirical work is largely based on an MTurk sample of individuals in the
United States. This being the case, I would be extremely interested in:

1.  Do these results hold in other countries across the world?
    Cross-cultural differences in scale-use are one hurdle to using such
    data for cross-country comparisons.

2.  Do these results hold in more traditional survey samples? One option
    here is to apply the proposed method to pre-existing data where
    traditional vignettes are available, such as the Gallup World Poll.
    Another option is to add calibration questions to, for example, the
    Innovation Sample of the German SOEP or the Dutch LISS panel.

3.  Do the results of Table 6 generalize to other wellbeing questions
    like happiness or worthwhileness? (Some such results are reported,
    but not on the regression adjustments).

## Conclusion

Notwithstanding the above comments, this is one of the most important
recent papers addressing the problem of interpersonal comparability of
survey data on wellbeing. The conclusions thus far suggest that
scale-use differences may not be as great an issue as previously feared
(with the implication that proposals like the WELLBY may be viable),
though there is clearly more theoretical and empirical work to be
done---which may well overturn this tentative empirical conclusion.

## References

[@nuq09uvozly] Daniel J. Benjamin, Kristen Cooper, Ori Heffetz, Miles S.
Kimball, and Jiannan Zhou, \"Adjusting for Scale-Use Heterogeneity in
Self-Reported Well-Being,\" NBER Working Paper 31728 (2023),
<https://doi.org/10.3386/w31728.>

[@nppib2p9wyx] Gary King, Christopher JL Murray, Joshua A Salomon, and
Ajay Tandon. Enhancing the validity and cross-cultural comparability of
measurement in survey research. American political science review,
98(1):191--207, 2004

[@n3lubycaawo] Teresa Bago d'Uva, Maarten Lindeboom, Owen O'Donnell, and
Eddy Van Doorslaer. Slipping anchor?: testing the vignettes approach to
identification and correction of reporting heterogeneity. Journal of
Human Resources, 46(4):875--906, 2011.

# Evaluator details

1.   What is your research field or area of expertise, as relevant to
    this research?

    -   Methods for subjective wellbeing research\

2.  How long have you been in your field of expertise?

    -   Since about 2018

3.  How many proposals, papers, and projects have you evaluated/reviewed
    (for journals, grants, or other peer-review)?

    -   10+

[^1]: Note: if you are reading this before, or soon after this has been
    publicly released, the ratings from this paper may not yet have been
    incorporated into that data presentation.

[^2]: We asked them to rank this paper "heuristically" as a percentile
    "relative to all serious research in the same area that you have
    encountered in the last three years." We requested they "consider
    all aspects of quality, credibility, importance to knowledge
    production, and importance to practice.

[^3]: See ranking tiers discussed
    [here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#journal-ranking-tiers).

[^4]: This comes from the form. If the author didn't do this, please
    skip this section.

[^5]: The evaluator was given the following instructions: Identify the
    most important and impactful factual claim this research makes --
    e.g., a binary claim or a point estimate or prediction.

    Please state the authors' claim precisely and quantitatively.
    Identify the source of the claim (i.e., cite the paper), and briefly
    mention the evidence underlying this. We encourage you to explain
    why you believe this claim is important, either here, or in the text
    of your report.

[^6]: "Feel free to express this in terms of the probability of the
    claim being true or as a credible interval for the parameter being
    estimated."
