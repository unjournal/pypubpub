---
affiliation:
- id: 0
  organization: UCL
article:
  elocation-id: e2heterogenity
author:
- Alberto Prati
bibliography: /tmp/tmp-60TznmAMUlu2x6.json
copyright:
  link: "https://creativecommons.org/licenses/by/4.0/"
  text: Creative Commons Attribution 4.0 International License
  type: CC-BY
csl: /app/dist/server/server/utils/citations/citeStyles/apa-6th-edition.csl
journal:
  publisher-name: The Unjournal
  title: The Unjournal
link-citations: true
title: Evaluation 2 of "Adjusting for Scale-Use Heterogeneity in
  Self-Reported Well-Being"
uri: "https://unjournal.pubpub.org/pub/e2heterogenity"
---

# Abstract 

This is an extraordinary paper [@n33q7yw2qux]. It approaches a
fundamental issue in wellbeing measurement, and does so constructively,
by suggesting and testing a potential solution.

# Summary Measures

We asked evaluators to give some overall assessments, in addition to
ratings across a range of criteria. *See the *[*evaluation summary
"metrics"*](https://unjournal.pubpub.org/pub/evalsumheterogenity#metrics "null")*
for a more detailed breakdown of this. See these ratings in the context
of all Unjournal ratings, with some analysis, in our *[*data
presentation
here.*](https://unjournal.github.io/unjournaldata/chapters/evaluation_data_analysis.html#basic-presentation "null")[^1]*
*

+-------------------+-------------------+---+
|                   | **Rating**        | * |
|                   |                   | * |
|                   |                   | 9 |
|                   |                   | 0 |
|                   |                   | % |
|                   |                   | C |
|                   |                   | r |
|                   |                   | e |
|                   |                   | d |
|                   |                   | i |
|                   |                   | b |
|                   |                   | l |
|                   |                   | e |
|                   |                   | I |
|                   |                   | n |
|                   |                   | t |
|                   |                   | e |
|                   |                   | r |
|                   |                   | v |
|                   |                   | a |
|                   |                   | l |
|                   |                   | * |
|                   |                   | * |
+===================+===================+===+
| **Overall         | 95/100            | 9 |
| assessment **     |                   | 0 |
|                   |                   |   |
|                   |                   | - |
|                   |                   | 1 |
|                   |                   | 0 |
|                   |                   | 0 |
+-------------------+-------------------+---+
| **Journal rank    | 4.8/5             | 4 |
| tier, normative   |                   | . |
| rating**          |                   | 5 |
|                   |                   |   |
|                   |                   | - |
|                   |                   | 5 |
|                   |                   | . |
|                   |                   | 0 |
+-------------------+-------------------+---+

**Overall assessment **(See footnote[^2])

**Journal rank tier, normative rating (0-5): ** On a 'scale of
journals', what 'quality of journal' should this be published in?[^3]
*Note: 0= lowest/none, 5= highest/best. *

# Claim identification and assessment [^4]

## I. Identify the most important and impactful factual claim this research makes[^5] {#i-identify-the-most-important-and-impactful-factual-claim-this-research-makes}

The authors develop an innovative framework to model and adjust for
scale heterogeneity, they test it using some new calibration questions
and show that their adjustment can lead to different results in some
applications.

## II.  Suggested robustness checks[^6] {#ii-suggested-robustness-checks}

From the current WP, it is not entirely clear how good the correction
performs when only two CQ are used and when short SWB scales are used.

# Written report

## Context

The authors use a psychometric approach to wellbeing measurement (in the
sense of Angner, 2009 [@nmbhf9a4sgl]), that is, they assume that there
exists a latent continuous construct representing the mental state that
we are actually interested in, and that the mental state is then
reported on an observable scale. This is a two-step process, where in
the first step people form a mental state based on some stimuli (e.g.,
what's going in their lives) and in the second step, they report it on a
scale. Both steps are non-observable, hence the problem of putting them
apart.

For most practical purposes, the analysis of subjective wellbeing data
requires a few assumptions about the reporting function: (1)
Comparability across individuals/groups (this paper); (2) Comparability
over time (see Prati and Senik, 2025 [@nzg65sd2jth]); (3) Linearity of
scale use (see Kaiser and Lepinteur, 2025 [@ng5pkgo8ru0]). These
assumptions are known to be violated to some extent. The important
challenges are to assess how consequential these violations are and how
we can correct for them. Potentially, the paper by Benjamin et al.
represents the most important contribution for assessing and solving
inter-personal heterogeneity in the use of SWB scales since the
introduction of anchoring vignettes twenty years ago.

My evaluation is based on both the NBER working paper and the
forthcoming version of the study which was presented at the Oxford
Wellbeing Research Centre six months agow. They are available,
respectively, at the links below:

[**https://www.nber.org/papers/w31728**](https://www.nber.org/papers/w31728 "null")

[**https://www.youtube.com/watch?v=RXXoCrmQhHE**](https://www.youtube.com/watch?v=RXXoCrmQhHE "null")

## Overall evaluation

This is an extraordinary paper. It is the kind of methodological
research that one wants to see more often. It approaches a fundamental
issue in wellbeing measurement, and does so constructively, by
suggesting and testing a potential solution.

The contribution is strong in both its theoretical and empirical parts.
As for the former, the authors offer a deep reflection on the problem of
scale use heterogeneity, connect it with the literature in the social
sciences, give a theoretically informed account of how to think about it
and suggest a sound solution for estimation. The empirical effort is
impressive too. The analysis in the working paper provides a useful
proof of concept, which has been supplemented by additional data from a
large representative sample (Understanding America Study) in the
unpublished forthcoming version. ¬†

The model is very well thought out. The use of a shifter and a stretcher
parameter makes a lot of sense. Some choices might go unnoticed to an
unfamiliar reader, but things like recentring the shifter, conditioning
the results on a questions' "height"[^7] and the distinction between
"dimensional scale use" and "general scale use" are actually smart
innovations. The adoption of a linear translation function seems
reasonable, and the analysis of some separate datasets in the new
version of the manuscript offers support for it. The assumption of a
unique reporting function for physical stimuli and cognitive assessments
across domains (i.e., assumption 3) is certainly not innocuous, but no
correction method is assumption-free, and the assumption seems
reasonable in many contexts and is discussed transparently.** **The
results in section VII additionally strengthen the paper. I am not the
best person to evaluate the details of section V ¬†\["Adjusting for
Scale-Use Heterogeneity" "Econometric Strategy", etc.\] and the related
appendix parts.

The heterogeneity of the reporting function is far from being a new
problem. If anything, it is something that has hindered the adoption of
subjective wellbeing data among economists, since corrections are rarely
used (see Fleurbaey and Blanchet, 2013 [@n0wmc1ss68r]). The main
contribution of this paper is to propose an instrument to (i) evaluate
and (ii) correct for it. ¬†

The empirical exercise gives some pretty good news for SWB
practitioners. Most of the heterogeneity in scale use comes from
differences across individuals rather than between sociodemographic
groups, and correcting for scale use heterogeneity does not flip
coefficient signs in a standard regression of life satisfaction on
socio-demographics. The bad news is that the estimation of uncorrected
coefficients sizes and coefficients ratios should be taken with a pinch
of salt, but this is not something new to SWB practitioners. I found the
implications for the use of fixed-effects and for the decomposition of
SWB variance (within and between) particularly thought-provoking. ¬†

The main limits that I see in this study are that: (1) calibration
questions are a costly addition to large surveys; (2) results are
derived from unusually long SWB scales (0-100); (3) the method is
conceived for interpersonal corrections, and I am not sure it would be
fit for correcting scale changes over time.¬† ¬†

I mostly congratulate the authors for such brilliant, original and
constructive contribution to a complicated and important issue.

## More comments about the limits

I think the paper has some limits not because of any fault in the
methods or reasoning, but rather because a single study cannot solve all
problems of response scale heterogeneity. This enterprise is proper to a
research agenda, and the current paper already provides a substantial
leap forward for future explorations to start from. Below, I discussed
three potential limits.

1.  **Adding calibration questions is costly **\[so how well does a
    smaller number of questions perform?\][^8]

The evidence reported in the working paper and discussed in the talk is
based on the analysis of a large number of calibrating questions (CQ).
The authors are very clear that they do not expect large surveys to
include tens of CQ, but:

-   From the current WP, it is not entirely clear how good the
    correction performs when only two or three CQ are used (i.e., the
    realistic scenario).

-   Already introducing two CQ can represent a substantial burden in
    large surveys, given the very tight space constraints, and could be
    cognitively demanding. I suspect this is one of the crucial reasons
    why anchoring vignettes have not been implemented at scale in these
    20 years.

I appreciated the discussion in Section V, that anticipates a criticism
to the econometric approach, i.e., the performance of an estimation
based on few CQ. The section witnesses both the intellectual rigor and
the attention to practitioners of the paper. However, in my
understanding, the empirical application in section VI is still run
using all the calibration questions. What happens if only visual or
non-visual CQ are used? ¬†

2.  **SWB scales are usually different from the ones used in the paper
    **\[0-100 vs. shorter scales\]

The empirical exercises are run using continuous 0-100 scales. These
scales are virtually absent from large surveys, which are the main
source of data for the SWB literature. Section VII.D shows that the
0-100 mean from calibration questions correlate with 0-10 (or smaller)
SWB scales, but some questions remain open, including:

-   Would the results of the rest of the paper be robust to the use of
    shorter scales?

-   Data collectors are rightfully reluctant to use different scales
    along the questionnaire. Therefore, would calibration questions that
    use the same short scale as the SWB one generate similar results?

3.  **The method is conceived for interpersonal corrections, and I am
    not sure it would be fit for correcting scale changes over time.¬†¬†**

To be clear, I don't think the authors ever claim that the method should
be used for correcting for rescaling, but I am discussing it because it
seems like a potential application of this method, and I would be a bit
sceptical about it. While response style - i.e., being more lenient vs
stringent, or having a tendency for moderate vs extreme responses -
seems, to a large extent, a fixed trait, the response function depends
not only on the response style, but also on what "0" and "10" of the
life satisfaction scale mean. Therefore, a person can report that their
life satisfaction is 8/10 for their whole life and that the size of the
circle is 8/10 for their whole life, yet they might undergo substantial
rescaling, so that the 8/10 of the life satisfaction scale is not
comparable over time. ¬†

Let me give you a concrete example from another setting, education,
where the reporting function that maps quality to grades has already
been associated with the reporting function for subjective wellbeing
(M√°rquez-Padilla and √Ålvarez, 2018 [@n66xktzys0k]). The same item - say,
an essay - can be graded differently by two teachers because, for
instance, they come from different countries and therefore have
different (stable) reporting styles. However, the same teacher might
grade the same essay differently from one year to another, not because
of any change in their reporting style, but because of a change in the
cohort they are benchmarking their assessment against.

The new version of the paper by Benjamin et al. adds a layer of details
in the theoretical part that goes a bit in this direction. The reporting
of the common scale SWB (ùë§~ùëñ~) is now modelled as a function of
auxiliary conditions ùúÜ~ùëñ~, something that was missing in the NBER
working paper. But since¬† ùúÜ~ùëñ~ is modelled as fixed over time, it does
not address the issue above, because benchmarks are likely to change
over time. ¬†

## References:

[@n76dpsilo1i] Daniel J. Benjamin, Kristen Cooper, Ori Heffetz, Miles S.
Kimball, and Jiannan Zhou, \"Adjusting for Scale-Use Heterogeneity in
Self-Reported Well-Being,\" NBER Working Paper 31728 (2023),
https://doi.org/10.3386/w31728.

[@na55vwuvsvt] Angner, Erik, Subjective Measures of Well‚ÄêBeing:
Philosophical Perspectives, in Don Ross, and Harold Kincaid (eds), *The
Oxford Handbook of Philosophy of Economics* (2009).

[@nrie7imygv1] Senik, Claudia, and Alberto Prati. \"Is it Possible to
Raise National Happiness?.\" *Working paper (2025)*

[@nvah3aew6i5] Kaiser, Caspar, and Anthony Lepinteur. \"Measuring the
Unmeasurable? Systematic Evidence on Scale Transformations in Subjective
Survey Data.\" Working paper (2025)¬†

[@n1rj62roknc] Fleurbaey, Marc, and Didier Blanchet. *Beyond GDP:
Measuring welfare and assessing sustainability*. Oxford University Press
(2013)

[@n2suc82jd4o] M√°rquez-Padilla, Fernanda, and Jorge √Ålvarez. \"Grading
happiness: What grading systems tell us about cross-country wellbeing
comparisons.\" Econ. Bull 38.2: 1138-55 (2018).

# Evaluator details

1.   What is your research field or area of expertise, as relevant to
    this research?

    -   Behavioral economics and subjective wellbeing measurement\

2.  How long have you been in your field of expertise?

    -   About 10 years

3.  How many proposals, papers, and projects have you evaluated/reviewed
    (for journals, grants, or other peer-review)?

    -   Between 40 and 60

[^1]: Note: if you are reading this before, or soon after this has been
    publicly released, the ratings from this paper may not yet have been
    incorporated into that data presentation.

[^2]: We asked them to rank this paper "heuristically" as a percentile
    "relative to all serious research in the same area that you have
    encountered in the last three years." We requested they "consider
    all aspects of quality, credibility, importance to knowledge
    production, and importance to practice.

[^3]: See ranking tiers discussed
    [here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#journal-ranking-tiers).

[^4]: This comes from the form. If the author didn't do this, please
    skip this section.

[^5]: The evaluator was given the following instructions: Identify the
    most important and impactful factual claim this research makes --
    e.g., a binary claim or a point estimate or prediction.

    Please state the authors' claim precisely and quantitatively.
    Identify the source of the claim (i.e., cite the paper), and briefly
    mention the evidence underlying this. We encourage you to explain
    why you believe this claim is important, either here, or in the text
    of your report.

[^6]: *We asked:*

    \[Optional\] What additional information, evidence, replication, or
    robustness check would make you substantially more (or less)
    confident in this claim?

    Feel free to refer to the main body of your evaluation here; you
    don\'t need to repeat yourself. Please specify how you would perform
    this robustness check (etc.) as precisely as you are willing. E.g.,
    if you suggest a particular estimation command in a statistical
    package, this could be very helpful for future robustness
    replication work.

[^7]: Managers' note, for context: According to the authors\
    "the amount of scale-use heterogeneity can vary with the average
    level of a survey question's responses, which we refer to throughout
    the paper as a question's height. If scale-use heterogeneity depends
    on an SWB question's height, then so does the appropriate scale-use
    adjustment.\"

[^8]: Manager: we added the elements in brackets to (hopefully) improve
    clarity.
